{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43f1a46-7004-4dbf-b365-0c19b3ef2700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 'model_state_dict' in checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2q/jvbp_2f501l86dgpwcfy90d80000gn/T/ipykernel_66614/3513390884.py:68: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1127 14:54:07.794000 66614 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `VisionTransformer([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `VisionTransformer([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n",
      "Failed to convert the model to the target version 17 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nizswan/miniforge3/envs/cv/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"/Users/nizswan/miniforge3/envs/cv/lib/python3.10/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"/Users/nizswan/miniforge3/envs/cv/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"/Users/nizswan/miniforge3/envs/cv/lib/python3.10/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: /Users/runner/work/onnx/onnx/onnx/version_converter/BaseConverter.h:68: adapter_lookup: Assertion `false` failed: No Adapter From Version $18 for Split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 27 of general pattern rewrite rules.\n",
      "Export Complete\n"
     ]
    }
   ],
   "source": [
    "#code for transforming from pth to onnx\n",
    "import torch\n",
    "import os\n",
    "#used timm to construct model\n",
    "import timm\n",
    "\n",
    "#original file pth\n",
    "pth_input = \"predictor.pth\"\n",
    "#new onnx\n",
    "onnx_output = \"predictor.onnx\"\n",
    "#number of labels in dataset\n",
    "class_count = 41\n",
    "#each image is 224x224\n",
    "dimension_size = 224\n",
    "\n",
    "#constructs a basic model\n",
    "def build_model(num_classes: int):\n",
    "    #constructs the model that I used (vit tiny), sets its parameters by new one with number of classes\n",
    "    model = timm.create_model(\n",
    "        \"vit_tiny_patch16_224\",\n",
    "        pretrained=False,\n",
    "        num_classes=num_classes,   # <-- FIXED to use the function arg\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "#loads the model from the checkpoint\n",
    "def load_checkpoint(path: str, model: torch.nn.Module):\n",
    "    #takes the checkpoint and loads it through torch to be able to adjust accordingly\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    #assistance with difference checkpoint types used for saving model\n",
    "    if isinstance(ckpt, torch.nn.Module):\n",
    "        print(\"[INFO] Loaded full model from checkpoint (torch.save(model, ...)).\")\n",
    "        return ckpt\n",
    "    #if the checkpoint is a dictionary instead of a model\n",
    "    if isinstance(ckpt, dict):\n",
    "        #check for the models state in model_state_dict and state_dict as precautionary measures\n",
    "        if \"model_state_dict\" in ckpt:\n",
    "            state_dict = ckpt[\"model_state_dict\"]\n",
    "            print(\"[INFO] Found 'model_state_dict' in checkpoint.\")\n",
    "        elif \"state_dict\" in ckpt:   # <-- FIXED syntax\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            print(\"[INFO] Found 'state_dict' in checkpoint.\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Checkpoint missing model weights\")\n",
    "        #loads the state from dictionary to model and returns\n",
    "        model.load_state_dict(state_dict)\n",
    "        #returns loaded model\n",
    "        return model\n",
    "    #raise error if any flags\n",
    "    raise RuntimeError(\"Checkpoint Issue with Model\")\n",
    "\n",
    "def main():\n",
    "    #checks to see if checkpoint exists, if not raises error\n",
    "    if not os.path.isfile(pth_input):   # <-- FIXED\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {pth_input}\")\n",
    "\n",
    "    #builds model\n",
    "    model = build_model(class_count)   # <-- FIXED\n",
    "\n",
    "    #loads weights onto the model\n",
    "    model = load_checkpoint(pth_input, model)  # <-- FIXED\n",
    "    model.eval()\n",
    "\n",
    "    #onnx needs dummy input to adapt and store information for one forward pass\n",
    "    dummy_input = torch.randn(1, 3, dimension_size, dimension_size, device=\"cpu\")\n",
    "\n",
    "    #exports to onnx, uses the model as base, saves name, gives example with a dummy input, etc.\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_output,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch\"},\n",
    "            \"logits\": {0: \"batch\"},\n",
    "        },\n",
    "        opset_version=17,\n",
    "    )\n",
    "    #logs completion of export\n",
    "    print(\"Export Complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273ccc65-ccc6-4693-90e8-026addc379a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX model\n",
      "Random test image: ../data/k1/k1_test/301_6183_001764.jpg\n",
      "True label ID: 301 (idx 17)\n",
      "True building: Engineering Buildings\n",
      "\n",
      "RESULTS\n",
      "Predicted index: 17\n",
      "Predicted label ID: 301\n",
      "Predicted building: Engineering Buildings\n",
      "\n",
      "Top-5 classes (idx, label_id, building, logit):\n",
      "  17 | 301 | Engineering Buildings | 16.067\n",
      "  19 | 303 | CREOL – College of Optics & Photonics | 4.002\n",
      "  22 | 403 | Theatre | 3.562\n",
      "   1 | 102 | Classroom Building 2 | 3.303\n",
      "  25 | 501 | John C. Hitt Library | 3.037\n"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import onnxruntime as ort\n",
    "\n",
    "#dummy script to pass\n",
    "model_dir = \"predictor.onnx\"\n",
    "image_dir = \"../data/k1/k1_test\"\n",
    "\n",
    "#all label ids for classification in my program\n",
    "indices = [\n",
    "    101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n",
    "    111, 112, 113,\n",
    "    201, 202, 203, 204,\n",
    "    301, 302, 303,\n",
    "    401, 402, 403, 404, 405,\n",
    "    501,\n",
    "    601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612,\n",
    "    701,\n",
    "    801, 802,\n",
    "]\n",
    "\n",
    "LABEL2IDX = {lab: i for i, lab in enumerate(indices)}\n",
    "IDX2LABEL = {i: lab for lab, i in LABEL2IDX.items()}\n",
    "\n",
    "#ids <-> names for easy human legibilitiy\n",
    "buildings = {\n",
    "    101: \"Classroom Building 1\",\n",
    "    102: \"Classroom Building 2\",\n",
    "    103: \"College of Arts and Humanities\",\n",
    "    104: \"Education Complex\",\n",
    "    105: \"Howard Phillips Hall\",\n",
    "    106: \"Math and Sciences Building\",\n",
    "    107: \"Nicholson School of Communication and Media\",\n",
    "    108: \"Teaching Academy\",\n",
    "    109: \"Trevor Colbourn Hall\",\n",
    "    110: \"Business Administration Buildings\",\n",
    "    111: \"Counseling and Psychology Services\",\n",
    "    112: \"College of Sciences Building\",\n",
    "    113: \"Burnett Honors College\",\n",
    "    201: \"Biological Sciences\",\n",
    "    202: \"Chemistry Building\",\n",
    "    203: \"Physical Sciences\",\n",
    "    204: \"Psychology Building\",\n",
    "    301: \"Engineering Buildings\",\n",
    "    302: \"L3Harris Engineering Center\",\n",
    "    303: \"CREOL – College of Optics & Photonics\",\n",
    "    401: \"Performing Arts – Music\",\n",
    "    402: \"Performing Arts – Theatre\",\n",
    "    403: \"Theatre\",\n",
    "    404: \"Rehearsal Hall\",\n",
    "    405: \"Visual Arts Building\",\n",
    "    501: \"John C. Hitt Library\",\n",
    "    601: \"Student Union\",\n",
    "    602: \"John T. Washington Center\",\n",
    "    603: \"63 South\",\n",
    "    604: \"Tech Commons Buildings\",\n",
    "    605: \"Health Center\",\n",
    "    606: \"General Ferrell Commons\",\n",
    "    607: \"Live Oak Event Center (Live Oak Ballroom)\",\n",
    "    608: \"Knights Pantry\",\n",
    "    609: \"Research 1\",\n",
    "    610: \"Career Services and Experiential Learning\",\n",
    "    611: \"FAIRWINDS Alumni Center\",\n",
    "    612: \"UCF Global\",\n",
    "    701: \"Millican Hall\",\n",
    "    801: \"Health Sciences I\",\n",
    "    802: \"Health Sciences II\",\n",
    "}\n",
    "\n",
    "#sets preprocessing based on image net, what the vit was pretrained on\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#forces to resize to 224, tensorize, and normalize for efficient processing\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "#takes the first part of the title of the image and classify as the true label\n",
    "def parse_label_from_filename(filename: str) -> int:\n",
    "    #takes the filename\n",
    "    base = os.path.basename(filename)\n",
    "    #splits the filename\n",
    "    stem, _ = os.path.splitext(base)\n",
    "    #takes the first chunk as in we have image input forms labelid_videoid_frameid\n",
    "    first_chunk = stem.split(\"_\")[0]\n",
    "    #returns true label\n",
    "    label_int = int(first_chunk)\n",
    "    return label_int\n",
    "    \n",
    "#picks a random image, \n",
    "def pick_random_image(img_dir: str):\n",
    "    #we only care for jpg in my images, for generlization it would be best to transfer from other images to jpg to match my model\n",
    "    exts = {\".jpg\"}\n",
    "    #collects all cadindates in the directory to see if they are jpg.\n",
    "    candidates = [os.path.join(img_dir, f)\n",
    "                  for f in os.listdir(img_dir)\n",
    "                  if os.path.splitext(f)[1].lower() in exts]\n",
    "    \n",
    "    #if none found raise error\n",
    "    if not candidates:\n",
    "        raise RuntimeError(f\"No images found in {img_dir}\")\n",
    "\n",
    "    #chooses a random one and returns it\n",
    "    path = random.choice(candidates)\n",
    "    return path\n",
    "\n",
    "#main\n",
    "def main():\n",
    "    #sets up the model/session\n",
    "    print(\"Loading ONNX model\")\n",
    "    sess = ort.InferenceSession(\n",
    "        model_dir,  # FIXED\n",
    "        providers=[\"CPUExecutionProvider\"],\n",
    "    )\n",
    "    #obtains inputs and outputs from model\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "    \n",
    "    #picks a random image\n",
    "    img_path = pick_random_image(image_dir)  # FIXED\n",
    "    #gets its out id\n",
    "    true_label_id = parse_label_from_filename(img_path)\n",
    "\n",
    "    #gets true label idx as it swaps from my hundreds calculator to #1-41\n",
    "    true_label_idx = LABEL2IDX[true_label_id]\n",
    "    true_name = buildings.get(true_label_id, f\"Building {true_label_id}\")  # FIXED\n",
    "\n",
    "    print(f\"Random test image: {img_path}\")\n",
    "    print(f\"True label ID: {true_label_id} (idx {true_label_idx})\")\n",
    "    print(f\"True building: {true_name}\")\n",
    "\n",
    "    #preprocesses the image, opens image in RGB, transforms image as necessary, and turns into vector for usage.\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img)\n",
    "    x = x.unsqueeze(0)\n",
    "    x_np = x.numpy().astype(np.float32)\n",
    "\n",
    "    #takes outputs by passing through model\n",
    "    outputs = sess.run([output_name], {input_name: x_np})\n",
    "    logits = outputs[0]\n",
    "    logits = logits[0]\n",
    "\n",
    "    #makes a prediction\n",
    "    pred_idx = int(np.argmax(logits))\n",
    "    pred_label_id = IDX2LABEL[pred_idx]\n",
    "    pred_name = buildings.get(pred_label_id, f\"Building {pred_label_id}\")  # FIXED\n",
    "\n",
    "    #prints results\n",
    "    print(\"\\nRESULTS\")\n",
    "    print(f\"Predicted index: {pred_idx}\")\n",
    "    print(f\"Predicted label ID: {pred_label_id}\")\n",
    "    print(f\"Predicted building: {pred_name}\")\n",
    "\n",
    "    #shows the five highest logits picked and their probability distribution\n",
    "    top5_idx = np.argsort(logits)[-5:][::-1]\n",
    "    print(\"\\nTop-5 classes (idx, label_id, building, logit):\")\n",
    "    for i in top5_idx:\n",
    "        lab_id = IDX2LABEL[int(i)]\n",
    "        name = buildings.get(lab_id, f\"Building {lab_id}\")  # FIXED\n",
    "        print(f\"  {int(i):2d} | {lab_id:3d} | {name} | {logits[i]:.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e526cd-08a9-4e79-92a0-083457c28702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductName:\t\tmacOS\n",
      "ProductVersion:\t\t13.2.1\n",
      "BuildVersion:\t\t22D68\n"
     ]
    }
   ],
   "source": [
    "!sw_vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e33c6-563e-4b3e-83c7-787cfe88ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip uninstall onnxruntime onnxruntime-silicon -y\n",
    "rm -rf ~/miniforge3/envs/cv/lib/python3.10/site-packages/onnxruntime*\n",
    "\n",
    "conda install -c conda-forge onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90bfb006-4934-4d60-b03d-00138a6feb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime version: 1.22.0\n",
      "Available providers: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"Available providers: {ort.get_available_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47629f29-73b7-4f87-a059-a3e4217366e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import onnxruntime as ort\n",
    "\n",
    "#path to the onnx model\n",
    "model_dir = \"predictor.onnx\"\n",
    "\n",
    "#all label ids for classification in my program\n",
    "indices = [\n",
    "    101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n",
    "    111, 112, 113,\n",
    "    201, 202, 203, 204,\n",
    "    301, 302, 303,\n",
    "    401, 402, 403, 404, 405,\n",
    "    501,\n",
    "    601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612,\n",
    "    701,\n",
    "    801, 802,\n",
    "]\n",
    "\n",
    "#mapping for idx <-> label\n",
    "LABEL2IDX = {lab: i for i, lab in enumerate(indices)}\n",
    "IDX2LABEL = {i: lab for lab, i in LABEL2IDX.items()}\n",
    "\n",
    "#names for buildings based on id\n",
    "buildings = {\n",
    "    101: \"Classroom Building 1\",\n",
    "    102: \"Classroom Building 2\",\n",
    "    103: \"College of Arts and Humanities\",\n",
    "    104: \"Education Complex\",\n",
    "    105: \"Howard Phillips Hall\",\n",
    "    106: \"Math and Sciences Building\",\n",
    "    107: \"Nicholson School of Communication and Media\",\n",
    "    108: \"Teaching Academy\",\n",
    "    109: \"Trevor Colbourn Hall\",\n",
    "    110: \"Business Administration Buildings\",\n",
    "    111: \"Counseling and Psychology Services\",\n",
    "    112: \"College of Sciences Building\",\n",
    "    113: \"Burnett Honors College\",\n",
    "    201: \"Biological Sciences\",\n",
    "    202: \"Chemistry Building\",\n",
    "    203: \"Physical Sciences\",\n",
    "    204: \"Psychology Building\",\n",
    "    301: \"Engineering Buildings\",\n",
    "    302: \"L3Harris Engineering Center\",\n",
    "    303: \"CREOL – College of Optics & Photonics\",\n",
    "    401: \"Performing Arts – Music\",\n",
    "    402: \"Performing Arts – Theatre\",\n",
    "    403: \"Theatre\",\n",
    "    404: \"Rehearsal Hall\",\n",
    "    405: \"Visual Arts Building\",\n",
    "    501: \"John C. Hitt Library\",\n",
    "    601: \"Student Union\",\n",
    "    602: \"John T. Washington Center\",\n",
    "    603: \"63 South\",\n",
    "    604: \"Tech Commons Buildings\",\n",
    "    605: \"Health Center\",\n",
    "    606: \"General Ferrell Commons\",\n",
    "    607: \"Live Oak Event Center (Live Oak Ballroom)\",\n",
    "    608: \"Knights Pantry\",\n",
    "    609: \"Research 1\",\n",
    "    610: \"Career Services and Experiential Learning\",\n",
    "    611: \"FAIRWINDS Alumni Center\",\n",
    "    612: \"UCF Global\",\n",
    "    701: \"Millican Hall\",\n",
    "    801: \"Health Sciences I\",\n",
    "    802: \"Health Sciences II\",\n",
    "}\n",
    "\n",
    "#preprocessing based on imagenet stats used by vit tiny\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#forces to resize to 224x224, convert to tensor, normalize\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "#convert png/webp/heic/etc to jpg for consistent model input\n",
    "def convert_to_jpg(path: str) -> str:\n",
    "    _, ext = os.path.splitext(path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    #jpg already correct input\n",
    "    if ext in [\".jpg\", \".jpeg\"]:\n",
    "        return path\n",
    "\n",
    "    #convert image to rgb then export as jpg\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    new_path = path + \".jpg\"\n",
    "    img.save(new_path, \"JPEG\")\n",
    "    return new_path\n",
    "\n",
    "#main classifier that takes a direct file path\n",
    "def main():\n",
    "    #ensures user provided an argument\n",
    "    if len(sys.argv) < 2:\n",
    "        raise RuntimeError(\"Please call: python generalclassifier.py <image_path>\")\n",
    "\n",
    "    #user provided image path\n",
    "    img_path = sys.argv[1]\n",
    "\n",
    "    #checks file exists\n",
    "    if not os.path.isfile(img_path):\n",
    "        raise FileNotFoundError(f\"File not found: {img_path}\")\n",
    "\n",
    "    #load ONNX session\n",
    "    print(f\"[INFO] Loading ONNX model from: {model_dir}\")\n",
    "    sess = ort.InferenceSession(\n",
    "        model_dir,\n",
    "        providers=[\"CPUExecutionProvider\"],\n",
    "    )\n",
    "\n",
    "    #get model input & output names\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "\n",
    "    #convert strange extension → jpg\n",
    "    img_path_jpg = convert_to_jpg(img_path)\n",
    "    print(f\"[INFO] Using processed image: {img_path_jpg}\")\n",
    "\n",
    "    #opens image for preprocessing\n",
    "    img = Image.open(img_path_jpg).convert(\"RGB\")\n",
    "\n",
    "    #apply 224x224 + normalize\n",
    "    x = transform(img)\n",
    "    x = x.unsqueeze(0)\n",
    "    x_np = x.numpy().astype(np.float32)\n",
    "\n",
    "    #run through model\n",
    "    outputs = sess.run([output_name], {input_name: x_np})\n",
    "    logits = outputs[0][0]\n",
    "\n",
    "    #get predicted class index\n",
    "    pred_idx = int(np.argmax(logits))\n",
    "    pred_label_id = IDX2LABEL[pred_idx]\n",
    "    pred_name = buildings.get(pred_label_id, f\"Building {pred_label_id}\")\n",
    "\n",
    "    #print results just like your UCF-specific classifier\n",
    "    print(\"\\nRESULTS\")\n",
    "    print(f\"Predicted index: {pred_idx}\")\n",
    "    print(f\"Predicted label ID: {pred_label_id}\")\n",
    "    print(f\"Predicted building: {pred_name}\")\n",
    "\n",
    "    #top 5 logits by descending order\n",
    "    top5_idx = np.argsort(logits)[-5:][::-1]\n",
    "\n",
    "    print(\"\\nTop-5 classes (idx, label_id, building, logit):\")\n",
    "    for i in top5_idx:\n",
    "        lab_id = IDX2LABEL[int(i)]\n",
    "        name = buildings.get(lab_id, f\"Building {lab_id}\")\n",
    "        print(f\"  {int(i):2d} | {lab_id:3d} | {name} | {logits[i]:.3f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
